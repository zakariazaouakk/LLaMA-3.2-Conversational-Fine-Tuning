{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqFtEr-Aibg4"
      },
      "outputs": [],
      "source": [
        "pip install unsloth transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN72YxUKi3m8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template, standardize_sharegpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLr_q0bQkLH5"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        " )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6LxacmIkoAn"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model, r=16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c17ztfBml0gm"
      },
      "outputs": [],
      "source": [
        "tokenizer = get_chat_template(tokenizer, chat_template=\"llama-3.1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htteoNMbl0jH"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")\n",
        "\n",
        "dataset = dataset.shuffle(seed=42).select(range(1000))\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "val_dataset = split_dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5pyic5al0lF"
      },
      "outputs": [],
      "source": [
        "# Apply standardization separately to train and validation\n",
        "train_dataset = standardize_sharegpt(train_dataset)\n",
        "val_dataset = standardize_sharegpt(val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm8jNgLUl0nS"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EUKbxWDmgxf"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]\n",
        "val_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgnYijk2mkhm"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    lambda examples: {\n",
        "        \"text\": [\n",
        "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
        "            for convo in examples[\"conversations\"]\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    lambda examples: {\n",
        "        \"text\": [\n",
        "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
        "            for convo in examples[\"conversations\"]\n",
        "        ]\n",
        "    },\n",
        "    batched=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sERgQz6vm-wx"
      },
      "outputs": [],
      "source": [
        "train_dataset\n",
        "val_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ89b247nB1W"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]\n",
        "val_dataset[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMMLtIlMY9gJ"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wao7Co0VZrFL"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0dEMeVjnDU_"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=10,\n",
        "        num_train_epochs=3,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=50,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-j71dmOofcH"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRnCv3LToi_q"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"finetuned_model1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE9KcYSpooip"
      },
      "outputs": [],
      "source": [
        "inference_model, inference_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"./finetuned_model1\",\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS3cj-YBWXde"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "small_val = val_dataset.select(range(min(50, len(val_dataset))))\n",
        "rouge_metric = evaluate.load(\"rouge\")\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"Calcul des scores ROUGE...\")\n",
        "for example in tqdm(small_val):\n",
        "    conv = example[\"conversations\"]\n",
        "\n",
        "    # Swapped 'value' for 'content'\n",
        "    prompt_text = conv[0][\"content\"]\n",
        "    target_text = conv[1][\"content\"]\n",
        "\n",
        "    formatted_prompt = inference_tokenizer.apply_chat_template([{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt_text\n",
        "    }], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    model_inputs = inference_tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = inference_model.generate(\n",
        "            **model_inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.1,\n",
        "            do_sample=False,\n",
        "            pad_token_id=inference_tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    input_len = model_inputs.input_ids.shape[1]\n",
        "    response = inference_tokenizer.decode(generated_ids[0][input_len:], skip_special_tokens=True)\n",
        "\n",
        "    all_preds.append(response)\n",
        "    all_labels.append(target_text)\n",
        "\n",
        "rouge_results = rouge_metric.compute(predictions=all_preds, references=all_labels)\n",
        "\n",
        "print(\"\\n--- Scores ROUGE Professionnels ---\")\n",
        "for key, value in rouge_results.items():\n",
        "    print(f\"{key.upper()}: {value*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcQyy5HFse18"
      },
      "outputs": [],
      "source": [
        "text_prompts = [\n",
        "    \"What are the key principles of investment?\"\n",
        "]\n",
        "\n",
        "for prompt in text_prompts:\n",
        "    formatted_prompt = inference_tokenizer.apply_chat_template([{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": prompt\n",
        "    }], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    model_inputs = inference_tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    generated_ids = inference_model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=inference_tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    response = inference_tokenizer.decode(generated_ids[0][model_inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Prompt: {prompt}\\n\")\n",
        "    print(f\"Assistant: {response}\\n\")\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}